---
title: Заплямованість та Толерантність
content_type: concept
weight: 50
---

<!-- overview -->
[_Спорідненість вузла (node affinity)_](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) це властивість {{< glossary_tooltip text="Podʼа" term_id="pod" >}}, яка _привертає_ Pod до набору {{< glossary_tooltip text="вузлів" term_id="node" >}} (або як перевага, або як жорстка вимога). _Заплямованість (taint)_ є протилежною властивістю — вона дозволяє вузлу відштовхувати набір Podʼів.

_Толерантність_ застосовуються до Podʼів. Толерантність дозволяє планувальнику розміщувати Podʼи, що збігаються з відповідними позначками заплямованості. Толерантність дозволяє планування, але не гарантує його: планувальник також [оцінює інші параметри](/docs/concepts/scheduling-eviction/pod-priority-preemption/) в межах свого функціонала.

Заплямованість та толерантність працюють разом, щоб забезпечити те, щоб Podʼи не розміщувались на непридатних вузлах. Одна або декілька позначок заплямованості застосовуються до вузла; це дозволяє позначити, що вузол не повинен приймати жодних Podʼів, які не толерують цих позначок заплямованості.

<!-- body -->

## Концепції {#concepts}

Ви додаєте позначку taint до вузла за допомогою [kubectl taint](/docs/reference/generated/kubectl/kubectl-commands#taint). Наприклад,

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
```

додає taint до вузла `node1`. Taint має ключ `key1`, значення `value1` і ефект taint `NoSchedule`. Це означає, що жодний Pod не зможе розміститись на `node1`, якщо він не має відповідної толерантності.

Щоб видалити taint, доданий командою вище, можна виконати:

```shell
kubectl taint nodes node1 key1=value1:NoSchedule-
```

Ви вказуєте толерантність для Podʼа в PodSpec. Обидві наступні толерантності "відповідають" taint, створеному за допомогою команди `kubectl taint` вище, і, отже, Pod з будь-якою з толерантностей зможе розміститись на `node1`:

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
```

```yaml
tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"
```

Основний планувальник Kubernetes бере до уваги taint та толерантності при виборі вузла для запуску певного Podʼа. Проте, якщо ви вручну вказуєте `.spec.nodeName` для Podʼа, ця дія оминає планувальник; Pod тоді привʼязується до вузла, на який ви його призначили, навіть якщо на цьому вузлі є taint типу `NoSchedule`, які ви обрали. Якщо це трапиться і на вузлі також встановлено taint типу `NoExecute`, kubelet видалятиме Pod, якщо не буде встановлено відповідну толерантність.

Ось приклад Podʼа, у якого визначено деякі толерантності:

{{% code_sample file="pods/pod-with-toleration.yaml" %}}

Типово значення для `operator` є `Equal`.

Толерантність має "збіг" з taint, якщо ключі є однаковими та ефекти є однаковими також, і:

* оператор — `Exists` (у цьому випадку не слід вказувати `value`), або
* оператор — `Equal`, а значення повинні бути рівні.

{{< note >}}

Є два спеціальних випадки:

Якщо `key` порожній, то `operator` має бути `Exists`, що відповідає всім ключам і значенням. Зверніть увагу, що `effect` все одно має збігатися з ключем.

Порожній `effect` відповідає всім ефектам з ключем `key1`.

{{< /note >}}

У вищенаведеному прикладі використовувався `effect` `NoSchedule`. Також можна використовувати `effect` `PreferNoSchedule`.

Дозволені значення для поля `effect`:

`NoExecute`
: Це впливає на Podʼи, які вже запущені на вузлі наступним чином:

* Podʼи, які не толерують taint, негайно виселяються
* Podʼи, які толерують taint, не вказуючи `tolerationSeconds` в їхній специфікації толерантності, залишаються привʼязаними назавжди
* Podʼи, які толерують taint з вказаним `tolerationSeconds` залишаються привʼязаними протягом зазначеного часу. Після закінчення цього часу контролер життєвого циклу вузла виводить Podʼи з вузла.

`NoSchedule`
: На позначеному taint вузлі не буде розміщено нові Podʼи, якщо вони не мають відповідної толерантності. Podʼи, які вже працюють на вузлі, **не** виселяються.

`PreferNoSchedule`
: `PreferNoSchedule` — це "preference" або "soft" варіант `NoSchedule`. Планувальник _спробує_ уникнути розміщення Podʼа, який не толерує taint на вузлі, але це не гарантовано.

На один вузол можна накласти декілька taint і декілька толерантностей на один Pod. Спосіб, яким Kubernetes обробляє декілька taint і толерантностей, схожий на фільтр: починаючи з усіх taint вузла, потім ігнорує ті, для яких Pod має відповідну толерантність; залишаються невідфільтровані taint, які мають зазначені ефекти на Pod. Зокрема,

* якщо є принаймні один невідфільтрований taint з ефектом `NoSchedule`, тоді Kubernetes не буде планувати Pod на цей вузол
* якщо немає невідфільтрованих taint з ефектом `NoSchedule`, але є принаймні один невідфільтрований taint з ефектом `PreferNoSchedule`, тоді Kubernetes **спробує** не планувати Pod на цей вузол
* якщо є принаймні один невідфільтрований taint з ефектом `NoExecute`, тоді Pod буде виселено з вузла (якщо він вже працює на вузлі), і він не буде плануватися на вузол (якщо він ще не працює на вузлі).

Наприклад, уявіть, що ви накладаєте taint на вузол таким чином

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
```

І Pod має дві толерантності:

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```

У цьому випадку Pod не зможе плануватися на вузол, оскільки відсутня толерантність, яка відповідає третьому taint. Але він зможе продовжувати працювати, якщо він вже працює на вузлі, коли до нього додається taint, оскільки третій taint — єдиний з трьох, який не толерується Podʼом.

Зазвичай, якщо до вузла додається taint з ефектом `NoExecute`, то будь-які Podʼи, які не толерують taint, будуть негайно виселені, а Podʼи, які толерують taint, ніколи не будуть виселені. Однак толерантність з ефектом `NoExecute` може вказати необовʼязкове поле `tolerationSeconds`, яке визначає, як довго Pod буде привʼязаний до вузла після додавання taint. Наприклад,

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
```

означає, що якщо цей Pod працює і до вузла додається відповідний taint, то Pod залишиться привʼязаним до вузла протягом 3600 секунд, а потім буде виселений. Якщо taint видаляється до цього часу, то Pod не буде виселений.

## Приклади використання {#example-use-cases}

Заплямованість та Толерантність є гнучким способом відвадити Podʼи *від* вузлів або виселення Podʼів, які не повинні працювати. Деякі з варіантів використання:

* **Призначені вузли**: Якщо ви хочете призначити певний набір вузлів для виключного використання конкретним набором користувачів, ви можете додати taint на ці вузли (наприклад, `kubectl taint nodes nodename dedicated=groupName:NoSchedule`) і потім додати відповідну толерантність до їхніх Podʼів (це найкраще робити за допомогою власного [контролера допуску](/docs/reference/access-authn-authz/admission-controllers/)). Podʼам з толерантностями буде дозволено використовувати позначені (призначені) вузли, а також будь-які інші вузли в кластері. Якщо ви хочете призначити вузли виключно для них _та_ забезпечити, що вони використовують лише призначені вузли, то ви також повинні додатково додати мітку, аналогічну taint, до того ж набору вузлів (наприклад, `dedicated=groupName`), і контролер допуску повинен додатково додати спорідненість вузла, щоб вимагати, щоб Podʼи могли плануватися лише на вузли з міткою `dedicated=groupName`.

* **Вузли зі спеціальним обладнанням**: У кластері, де невелика підмножина вузлів має спеціалізоване обладнання (наприклад, GPU), бажано утримувати Podʼи, які не потребують спеціалізованого обладнання, поза цими вузлами, щоб залишити місце для Podʼів, які дійсно потребують спеціалізованого обладнання. Це можна зробити, накладаючи taint на вузли зі спеціалізованим обладнанням (наприклад, `kubectl taint nodes nodename special=true:NoSchedule` або `kubectl taint nodes nodename special=true:PreferNoSchedule`) і додавання відповідної толерантності до Podʼів, які використовують спеціалізоване обладнання. Як і у випадку з призначеними вузлами, найпростіше застосовувати толерантності за допомогою власного [контролера допуску](/docs/reference/access-authn-authz/admission-controllers/). Наприклад, рекомендується використовувати [Розширені ресурси](/docs/concepts/configuration/manage-resources-containers/#extended-resources)
для представлення спеціального обладнання, позначайте вузли зі спеціальним обладнанням розширеним імʼям ресурсу і запускайте контролер допуску [ExtendedResourceToleration](/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration). Тепер, оскільки вузли позначені, жоден Pod без толерантності не буде плануватися на них. Але коли ви надсилаєте Pod, який запитує розширений ресурс, контролер допуску `ExtendedResourceToleration` автоматично додасть правильну толерантність до Podʼа і цей Pod буде плануватися на вузли зі спеціальним обладнанням. Це забезпечить, що ці вузли зі спеціальним обладнанням призначені для Podʼів, які запитують таке обладнання, і вам не потрібно вручну додавати толерантності до ваших Podʼів.

* **Виселення на основі taint**: Поведінка виселення, що налаштовується для кожного Podʼа, коли є проблеми з вузлом, яка описана в наступному розділі.

## Виселення на основі taint {#taint-based-evictions}

{{< feature-state for_k8s_version="v1.18" state="stable" >}}

Контролер вузла автоматично накладає taint на вузол, коли виконуються певні умови. Наступні taint є вбудованими:

* `node.kubernetes.io/not-ready`: Вузол не готовий. Це відповідає тому, що стан NodeCondition "`Ready`" є "`False`".
* `node.kubernetes.io/unreachable`: Вузол недоступний з контролера вузла. Це відповідає тому, що стан NodeCondition "`Ready`" є "`Unknown`".
* `node.kubernetes.io/memory-pressure`: Вузол має проблеми з памʼяттю.
* `node.kubernetes.io/disk-pressure`: Вузол має проблеми з диском.
* `node.kubernetes.io/pid-pressure`: Вузол має проблеми з PID.
* `node.kubernetes.io/network-unavailable`: Мережа вузла недоступна.
* `node.kubernetes.io/unschedulable`: Вузол не піддається плануванню.
* `node.cloudprovider.kubernetes.io/uninitialized`: Коли kubelet запускається з "зовнішнім" хмарним провайдером, цей taint накладається на вузол для позначення його як невикористовуваного. Після того як контролер з cloud-controller-manager ініціалізує цей вузол, kubelet видаляє цей taint.

У разі, якщо потрібно спорожнити вузол, контролер вузла або kubelet додає відповідні taint з ефектом `NoExecute`. Цей ефект типово додається для taint `node.kubernetes.io/not-ready` та `node.kubernetes.io/unreachable`. Якщо умова несправності повертається до нормального стану, kubelet або контролер вузла можуть видалити відповідні taint.

У деяких випадках, коли вузол недоступний, сервер API не може спілкуватися з kubelet на вузлі. Рішення про видалення Podʼів не може бути передане до kubelet до тих пір, поки звʼязок з сервером API не буде відновлено. Протягом цього часу Podʼи, які заплановані для видалення, можуть продовжувати працювати на розділеному вузлі.

{{< note >}}
Панель управління обмежує швидкість додавання нових taint на вузли. Це обмеження швидкості керує кількістю видалень, які спричиняються, коли одночасно недоступно багато вузлів (наприклад, якщо відбувається збій мережі).
{{< /note >}}

Ви можете вказати `tolerationSeconds` для Podʼа, щоб визначити, як довго цей Pod залишається привʼязаним до несправного або вузла, ще не відповідає.

Наприклад, ви можете довго зберігати застосунок з великою кількістю локального стану, привʼязаного до вузла, у разі розділу мережі, сподіваючись, що розділ відновиться, і, таким чином, можна уникнути виселення Podʼа. Толерантність, яку ви встановили для цього Pod, може виглядати так:

```yaml
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000
```

{{< note >}}
Kubernetes автоматично додає толерантність для `node.kubernetes.io/not-ready` та `node.kubernetes.io/unreachable` з `tolerationSeconds=300`, якщо ви або контролер встановили ці толерантності явно.

Ці автоматично додані толерантності означають, що Podʼи залишаються привʼязаними до вузлів протягом 5 хвилин після виявлення однієї з цих проблем.
{{< /note >}}

Podʼи [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) створюються з толерантностями `NoExecute` для наступних taint без `tolerationSeconds`:

  * `node.kubernetes.io/unreachable`
  * `node.kubernetes.io/not-ready`

Це забезпечує, що Podʼи DaemonSet ніколи не будуть видалені через ці проблеми.

## Позначення вузлів taint за умовами {#taint-nodes-by-conditions}

Панель управління, використовуючи {{<glossary_tooltip text="контролер" term_id="controller">}} вузла, автоматично створює taint з ефектом `NoSchedule` для [умов вузла](/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions).

Планувальник перевіряє taint, а не умови вузла, коли він приймає рішення про планування. Це забезпечує те, що умови вузла не впливають безпосередньо на планування. Наприклад, якщо умова вузла `DiskPressure` активна, панель управління додає taint `node.kubernetes.io/disk-pressure` і не планує нові Podʼи на уражений вузол. Якщо умова вузла `MemoryPressure` активна, панель управління додає taint `node.kubernetes.io/memory-pressure`.

Ви можете ігнорувати умови вузла для новостворених Podʼів, додавши відповідні толерантності Podʼів. Панель управління також додає толерантність `node.kubernetes.io/memory-pressure` на Podʼи, які мають {{< glossary_tooltip text="клас QoS" term_id="qos-class" >}} інший, ніж `BestEffort`. Це тому, що Kubernetes вважає Podʼи у класах QoS `Guaranteed` або `Burstable` (навіть Podʼи без встановленого запиту на памʼять) здатними впоратися з тиском на памʼять, тоді як нові Podʼи `BestEffort` не плануються на уражений вузол.

Контролер DaemonSet автоматично додає наступні толерантності `NoSchedule` для всіх демонів, щоб запобігти порушенню роботи DaemonSet.

  * `node.kubernetes.io/memory-pressure`
  * `node.kubernetes.io/disk-pressure`
  * `node.kubernetes.io/pid-pressure` (1.14 або пізніше)
  * `node.kubernetes.io/unschedulable` (1.10 або пізніше)
  * `node.kubernetes.io/network-unavailable` (_тільки для мережі хосту_)

Додавання цих толерантностей забезпечує сумісність з попередніми версіями. Ви також можете додавати довільні толерантності до DaemonSets.

## {{% heading "whatsnext" %}}

* Дізнайтеся про [Виселення через тиск на вузол](/docs/concepts/scheduling-eviction/node-pressure-eviction/) та як його налаштувати
* Ознайомтесь з [Пріоритетом Podʼа](/docs/concepts/scheduling-eviction/pod-priority-preemption/)
